\section{Related Work} \label{sect:related-work}


Recent surveys:
- verifiable computing~\cite{WB13} 
- general verifiable cloud services~\cite{BCC+13}, 
- cloud monitoring~\cite{ABD+13,FEH+14}

-----------------------------

\subsection{Storage}

Early proof-of-storage systems were proposed by Golle, Jarecki, and Mironov~\cite{GJM02}, Deswarte, Quisquater, and Sa\"{\i}dane~\cite{DQS03}, Filho and Barreto~\cite{FB06}, and Schwarz and Miller~\cite{SM06}.
However, these proposals lacked a formal security model and analysis.
Juels and Kaliski~\cite{JK07}, Naor and Rothblum~\cite{NR09} considered the first formal models in the context of authenticators and proofs of retrievability (POR), respectively.
Subsequently, a series of improvements have been proposed, for example, in terms of public verifiability~\cite{AKK09,ABC+11}, compactness of proofs~\cite{SW13}, support for dynamic data update~\cite{ADM+08,EKP+09,WWL+09,SDJ+12,CKW13,SSP13}, replication~\cite{CKB+08}, and security against a fully Byzantine adversarial model~\cite{BJO09a}.

There also exist proposals distributed proof-of-storage systems.
Bowers, Juels, and A. Oprea~\cite{BJO09b} proposed HAIL, in which files are divided in segments and stored across a collection of distributed servers.
They showed that the integrity and availability of files can be realized using file authentication techniques and integrity-protected error-correcting code in an interactive and synchronous manner among the distributed servers.
Zhu et al.~\cite{ZHA+12} considered a distributed proof-of-data-possession (PDP) system in a multi-cloud model, which requires a trusted third party to store public parameters used for proof verification.
On the other hand, Etemad and K{\"u}p\c{c}{\"u}~\cite{EK13} proposed a distributed and replicated dynamic PDP system which is transparent from the user's viewpoint (in terms of the internal architecture of the cloud provider).
In very recent development, Miller et al.~\cite{MJS+14} show that a distributed POR system can be adapted for mining Bitcoin (instead of using computational resources).

\paragraph{Possession.}

\paragraph{Retrievability.}

\paragraph{Redundancy.}




\subsection{Computation}

\paragraph{Correctness.}

\paragraph{Effort.}

[General proof of work]

[Proof systems]
Various general-purpose proof systems, pioneered by the work of Babai~\cite{Bab85} and Goldwasser et al.~\cite{GMR89}, have been proposed for verifying computation of arbitrary functions.
Proposals in this line of research are largely theoretical.
However, recent promising results on more efficient proof systems, e.g., Pinocchio~\cite{PHG+13} and TrueSet~\cite{KPP+14}, give a glimpse of hope for possible practical deployment of these systems.
Nevertheless, further refinements and research are required before they become truly practical.

%\paragraph{Interactive Proofs.}
Earlier theoretical proof systems involve interaction between a prover and a verifier.
Interactive proofs, such as~\cite{LFK+92,Sha92}, allow the prover to probabilistically convince the verifier of the truth of any function computable in polynomial time.
Typically the prover runs in super-polynomial time, while the verifier runs in polynomial time.
Nevertheless, Goldwasser et al.~\cite{GKR08} showed that for computation of small space or depth, it is possible to reduce the prover complexity for interactive proofs to polynomial time, and the verifier complexity to almost linear time.  

From interactive proof systems, one can derive {\em probabilistically checkable proofs} (PCPs), allowing the verifier to randomly check only a small fraction of the proof, e.g., a constant number of bits of the proof for NP languages.
However, this typically requires the prover to provide a long PCP (of polynomial length) to the verifier.
Kilian~\cite{Kil92} and Micali~\cite{Mic94} showed that {\em interactive arguments} (aka computationally sound proofs), in which the prover is assumed to be computationally bounded, can be used to alleviate such an issue.
Particularly, they gave constant-round protocols with a polynomial-time prover and an almost linear-time verifier, assuming the existence of collision-resistant functions.

A series of recent work has considerably improved the efficiency of interactive proof-based verifiable computation, see for example~\cite{CMT12,SMB+12,SVP+12,TRM+12,VSB+13}.
However, these proposals are still not practical.

%\paragraph{Non-interactive Proofs.}
Micali~\cite{Mic94} also suggested that a non-interactive proof system (with roughly similar efficiency to that of the interactive version) is possible in the random oracle model~\cite{BR93}.
However, the random oracle heuristic is known to be unsound in general~\cite{CGH04}.
More recently, Goldwasser et al.~\cite{GKR08} showed that interactive proofs for small-depth computation can be converted into non-interactive arguments, assuming the existence of single-server private information retrieval (PIR)~\cite{CKG+98}.
Nevertheless, such arguments are applicable to only a restricted class of functions.

Gennaro et al.~\cite{GGP10} recently showed that it is possible to verify any arbitrary computation by increasing the verifier offline complexity and making use of fully homomorphic encryption (FHE)~\cite{Gen09}.
They observed that Yao's garbled circuit construction~\cite{Yao82} provides a ``one-time'' verifiable computation, in addition to allowing secure two-party computation.
Hence, Yao's construction can be adapted to allow a client to outsource the computation of a function to a worker, while allowing the client to verify the correctness of the output of the computation.
However, reusing the same circuit for different input is insecure, and thus, Gennaro et al.\ proposed the combined use of a garbled circuit and FHE, such that the circuit is evaluated by the worker in an encrypted domain.
This way, the labels associated with the bits of client's input is not leaked and can be reused many times.
The use of FHE ensures input and output privacy for the client.
To evaluate a function $F:\{0,1\}^n \to \{0,1\}^m$, which can be described by a Boolean circuit $C$, the Gennaro et al.\ VC scheme requires one-time (offline)  preprocessing step that takes $O(|C|)$ time, i.e., comparable to computing the function from scratch.
However, the one-time cost can be amortized over many executions on different input choices for the same function.
At the online stage, the client runs in linear time, that is, input preparation takes $O(n)$ time and output verification takes $O(m)$ time.
On the other hand, the worker takes $O(|C|)$ to compute the function for the client.
Subsequently Chung et al.~\cite{CKV10} proposed a VC scheme (also with input and output privacy) which reduces the preprocessing cost to $O(\log(|C|))$, but requiring an additional round of communication between the client and the worker.

However, FHE-based VC schemes have an inherent limitation.
The output of the verification check performed by the client must not be revealed to the worker; otherwise a malicious worker may learn a bit of information about the labels of the evaluated garbled circuit.
This is known as the ``rejection problem''.\footnote{A malicious cloud that is able to observe the result of the verification procedure, i.e., accept/reject decision, on polynomially many inputs can eventually break the soundness of the scheme~\cite{PRV12}.}
Various approaches have been proposed to address this, such as public VC (supporting public delegatability and verifiable)~\cite{PRV12,PST13}, memory delegation~\cite{CKL+11}, combined PCP and PIR techniques~\cite{BCC+12}, combined FHE and function encryption~\cite{BF12}.

Parno et al.~\cite{PHG+13} recently proposed Pinocchio, a system for public verifiable general-purpose computation with orders of magnitude efficiency improvement over previous proposals.
Their solution make use of {\em quadratic programs}~\cite{GGP+13}, which allows compact encoding of computation from an associated arithmetic or Boolean circuit, in order to speed up proof generation and verification.
The key setup and proof generation require cryptographic effort linear in the size of the original computation, and verification requires time linear in the size of the input and output.
Kosba et al.~\cite{KPP+14} proposed {\sc TrueSet}, which further improves the efficiency of Pinocchio.
Their core idea is to model computation as a {\em set circuit}, i.e., circuit on polynomials, which can be encoded as quadratic polynomial programs.
This is then combined with {\em succinct non-interactive argument of knowledge} (SNARK)~\cite{GGP+13} for polynomial circuits to achieve input-specific running time during key setup and proof generation.
{\sc TrueSet} supports arbitrary computation tasks since the set circuit supports intersection, union, and difference gates.
However, both Pinocchio and {\sc TrueSet} do not preserve input and output privacy.

There also exist other efficient VC solutions but supporting smaller classes of functions, polynomials and set operations, see for example~\cite{BGV11,PTT11,FG12}.
Publicly verifiable set operations over outsourced and {\em authenticated} datasets are considered in~\cite{CPP+14}.

Recently Fiore, Gennaro, and Pastro~\cite{FGP14} developed a new homomorphic hashing technique that allows efficient verifiable computation...

------------

There exist several reasonably efficient and practical methods for verifying general secure computation on data outsourced by a client to one or multiple workers (or servers).

[Replication]
One natural and simple way for the client to verify the correctness of computation is to request multiple workers to execute its program on the same dataset. The client then takes the plurality value of the results from the workers to be the correct answer. As long as a majority of the workers are honest, the client gets the correct answer. In case there is inconsistency, the client can execute the program itself. Typically, the client and the workers sign all messages exchanged between them. This is to prevent sybil and impersonation attacks, for example.

The need for an honest majority of workers can be obviated using refereed delegation of computation~\cite{CRR11}. In this computation model, the client requests for the result of a function from two (or more) workers. In case of inconsistency, the client can efficiently determine the true claim as long as there is at least one honest worker. Here, the client acts as a ``referee'' and is not required to perform the verification itself. The overhead (in the event of inconsistency or dispute) is poly-logarithmic in the size of the computation. It has been shown in~\cite{CRR11} that, for some parameters, one gets an 8x slowdown in a program for computing the determinant of a given matrix.

Alternatively, the client can establish a contractual agreement with two (or more) workers in one of the following two ways~\cite{NK12}:
\begin{itemize}
 \item If the client asks for computation results from both workers, and the results do not match, both workers pay a penalty to the client, and return the money paid for the computation as well.
 \item If the client asks for computation results from both workers, and the results do not match, the client performs a potentially costly audit of the computation. Each worker whose result does not match the result given by the client's audit pays a fine to the client.
\end{itemize}
The above two agreements can be proved in a game-theoretic model that they have individually rational and incentive compatible equilibrium, assuming the workers behave honestly. 
It was recently shown in~\cite{CKP14} that optimal results (in terms of the client's expected costs) can still be achieved even when considering side-channel information and collusion among workers.

Clearly, a limitation of the replication method is the need for at least two workers for any computational tasks. However, in return, the client is guaranteed of detecting any cheating worker.

[Random Spot-checking]
Another common approach is to insert fake, random items at random positions when forming a computational task for a worker~\cite{GM01,JK07}. The fake items need to be chosen only once for all tasks. Upon job completion, the client checks if the returned results corresponding to the fake items are as expected. The complexity of verification overhead is $O(n)$ for $n$ fake items, where n is chosen sufficiently large for achieving high probability of cheating detection. It was recently shown in~\cite{BZF13} that for secure (privacy-preserving) and verifiable outsourcing of Hamming distance computations, the worker's computational overhead increases only moderately, by roughly 10-24\% for different sizes of datasets after insertion of fake items. On the other hand, the client's preparation and verification overhead is roughly 8 seconds for a dataset of 8,000 items.

The spot-checking approach may be cheaper than using replication, with slightly weaker (probabilistic) guarantee of cheating detection.

[Obfuscation of Code]
One can also use code obfuscation techniques, e.g., to insert checkable state points in computational tasks, such that the worker is able to produce proofs of execution comprising a sequence of state points, which can be verified by the worker~\cite{Hoh98, MWR99}.

[Trusted Hardware]
The client may also use trusted hardware, e.g., tamper-proof co-processor, on the server to prove the correctness of the computation~\cite{SPD05}. This is somewhat conceptually similar to relying on a weak trusted third party, i.e., hardware manufacturer.

Kumaresan and Bentove~\cite{KB14} recently proposed a model of incentivizing correct computation using Bitcoin. 


\subsection{Data Transfer}

Verifiable Content Distribution. It was shown in~\cite{KFM04} that verification of erasure-encoded data blocks in P2P-CDNs can be performed on-the-fly, i.e., recipients can verify the integrity of check blocks immediately as each data block is downloaded. Such a scheme can be constructed based on a homomorphic hash function.

File sharing or content distribution is commonly done over a peer-to-peer (P2P) network. However, a fundamental problem in a P2P network is the so-called ``free riders'' problem, i.e., users who use the services provided by others but do not provide any services to their peers. 
Motivated by this problem, Golle, Jarecki, and Mironov~\cite{GJM02} introduced two cryptographic primitives:
communication-enforcing signature (CES), which enforces ``proofs-of-download'' or ``pay-for-bandwidth'' in the context of P2P content distribution networks (P2P-CDNs);
storage-enforcing commitment (SEC), which enforces ``proofs-of-storage'' or ``proofs-of-data-possession''.
Standard signatures are not adequate as proofs-of-download, since the hash-and-sign paradigm implies that knowledge of the digest of a message is sufficient to sign the whole message. Hence, a dishonest user can save on bandwidth by exchanging only the hashes of requested content and claim credit for content she has never downloaded. CES works in such a way that the signer can only sign a message after exchanging at least as much data with the requester as would be required to send the entire message. On the other hand, SEC is based on a standard commitment scheme with the additional property that the party holding a commitment can ask the party that committed itself to a message to prove that it is still in the possession of the message.

Subsequently in~\cite{FB06}, a CES scheme in a stronger security model, which assumes both uploaders and downloaders are untrusted, was proposed. A SEC scheme with smaller public key size was also proposed.



\subsection{Location}

~\cite{BDS11,WSA+12}

\subsection{Isolation}

\paragraph{VM.}
~\cite{ZJO+11}

\paragraph{Physical Disks.}

~\cite{WSJ+12,WSJ+13,WSJ+14}

\subsection{Fault Tolerance}

~\cite{BDJ+11}

\subsection{Space}

There also exist primitives to enable a client to verify if the cloud has the requested amount of memory space~\cite{ABF13} and disk space~\cite{DFK+13}.

\subsection{Erasure}

~\cite{PT10}




